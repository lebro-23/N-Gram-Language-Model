{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JXuab6JpneJy"
   },
   "source": [
    "# N-gram Language model\n",
    "\n",
    "\n",
    "In this milestone you will build a language model based on the corpus you prepared in milestone 1. \n",
    "\n",
    "In milestone 2, we will work on the following items:\n",
    "\n",
    "* Calculating the probabilities of n-grams using Maximum Likelihood Estimation and the equation: \n",
    "$$p(token / prefix) = \\frac{count(prefix + token)} {count(prefix)}  $$\n",
    "\n",
    "* Creating a text generator function based on the n-gram model\n",
    "* Generating text with temperature sampling\n",
    "\n",
    "\n",
    "The goal is to have a language model that can generate text.\n",
    "\n",
    "\n",
    "In the next milestone we will learn how to assess the quality of our langauge model and handle words not present in the original corpus.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "sfXBcMPOpJ02"
   },
   "source": [
    "# Loading the dataset\n",
    "\n",
    "Let's import the library and load the dataset that we created in the previous task\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "H69DABm5qMgt"
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import csv\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.util import ngrams\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import random"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yuV6ny4UvZqf"
   },
   "outputs": [],
   "source": [
    "# if there's a problem with the versions of the librairies, you can . . uncomment this line and install the proper versions\n",
    "\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "wV1Qzr-TrYas"
   },
   "outputs": [],
   "source": [
    "# Set some global parameters\n",
    "\n",
    "# Displaying all columns when displaying dataframes\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# We will work with trigrams \n",
    "ngrams_degree = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fHoDlqr_qPub"
   },
   "outputs": [],
   "source": [
    "# Load data into pandas dataframe, shuffle it and reset the index\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 399
    },
    "colab_type": "code",
    "id": "4xS79J8OekRl",
    "outputId": "b3884596-2ad1-4bc5-b87b-40d4f03d28c0"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(789649, 7)\n",
      "\n",
      "   post_id  parent_id  comment_id  \\\n",
      "0   161009        NaN    309845.0   \n",
      "1   156252        NaN    298634.0   \n",
      "2   423360        NaN    790161.0   \n",
      "3   268623        NaN         NaN   \n",
      "4   433662        NaN    808873.0   \n",
      "\n",
      "                                                text category  \\\n",
      "0  I can't disclose the algorithm, but I can cert...  comment   \n",
      "1  I plan to leave the answer to this question in...  comment   \n",
      "2  Wait, I need to clarify how is Half-normal dis...  comment   \n",
      "3  I am fitting several models of the form.. glm ...     post   \n",
      "4  If you really want to calculate some p-value u...  comment   \n",
      "\n",
      "                                              tokens  n_tokens  \n",
      "0  i can ' t disclose the algorithm , but i can c...        40  \n",
      "1  i plan to leave the answer to this question in...        84  \n",
      "2  wait , i need to clarify how is half - normal ...        25  \n",
      "3  i am fitting several models of the form .. glm...        82  \n",
      "4  if you really want to calculate some p - value...        66  \n"
     ]
    }
   ],
   "source": [
    "# Display the dimensions of the dataframe \n",
    "\n",
    "# And the 1st 5 lines\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "TL9EncUpxTN4"
   },
   "source": [
    "The tokens were saved as strings with space separated tokens for simplicity purposes. \n",
    "\n",
    "Let's transform the tokens colum as an actual list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "T3eW9r6erVG1"
   },
   "outputs": [],
   "source": [
    "#Change tokens column back to a list of tokens from a space separated string\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "pLajyrl-sOey",
    "outputId": "89672f76-cf37-45ff-e57d-0625130292a7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([list(['do', 'you', 'see', 'any', 'obvious', 'relation', 'to', 'the', 'wald', '-', 'confidence', 'interval', '?']),\n",
       "       list(['should', 'i', 'create', 'categories', '?', 'if', 'not', 'how', 'can', 'i', 'interpret', 'odds', 'ratio', 'while', 'there', 'is', 'no', 'reference', 'category', '?']),\n",
       "       list(['experiment', 'design', 'process', 'outputs', 'the', 'order', 'in', 'which', 'to', 'do', 'the', 'work', 'assumption', 'ordering', 'of', 'work', 'items', 'is', 'most', 'preferred', 'valuable', 'by', 'stakeholders', 'work', 'items', 'the', 'process', 'above', 'produces', 'a', 'rank', 'ordering', 'of', 'items', '.', 'individual', 'teams', 'select', 'work', 'items', 'from', 'this', 'prioritized', 'queue', 'to', 'do', 'their', 'work', '.', 'i', 'introduced', 'a', 'new', 'process', 'to', 'perform', 'the', 'prioritization', 'and', 'we', 'did', 'notice', 'some', 'improvement', 'in', 'the', 'selection', 'of', 'work', 'items', 'i', '.', 'e', '.,', 'work', 'items', 'were', 'more', 'and', 'more', 'goal', 'oriented', 'and', 'not', 'as', 'per', 'whims', 'of', 'individuals', 'or', 'star', 'performers', 'only', '.', 'question', 'to', 'answer', 'given', 'year', 'experimental', 'data', 'i', 'wish', 'to', 'know', 'if', 'my', \"'\", 'new', \"'\", 'process', 'was', 'any', 'better', '?', 'current', 'approach', 'for', 'a', 'set', 'of', 'work', '-', 'items', 'i', 'compute', 'how', 'strongly', 'did', 'their', 'selection', 'choice', 'compare', 'with', 'the', 'prioritization', 'ordering', 'of', 'the', 'new', 'old', 'process', '.', 'i', 'compute', 'a', 'correlation', 'metric', 'almost', 'like', 'an', 'exam', 'score', 'in', 'essence', 'which', 'is', 'a', 'weighted', 'average', 'of', 'the', 'proportion', 'of', 'work', '-', 'items', 'completed', '.', 'for', 'example', 'percentage', 'of', 'top', 'work', 'items', 'completed', '?', 'note', 'for', 'simplicity', 'we', 'are', 'considering', 'only', 'one', 'iv', 'since', 'most', 'of', 'the', 'other', 'variables', 'is', 'what', 'we', 'controlled', 'for', 'keeping', 'them', 'almost', 'constant', 'i', '.', 'e', '.,', 'team', 'size', 'composition', 'etc', '.,', 'and', 'we', 'people', 'reviewing', 'this', 'experiment', 'have', 'agreed', 'that', 'they', 'don', \"'\", 't', 'impact', 'the', 'dv', 'and', 'can', 'thus', 'be', 'removed', 'from', 'the', 'model', '.']),\n",
       "       list(['you', 'write', 'i', 'think', 'it', 'unlikely', 'would', 'be', 'a', 'reasonable', 'answer', 'due', 'to', 'its', 'asymmetric', 'treatment', 'of', 'the', 'extremes', '.', '--', 'can', 'you', 'elaborate', 'on', 'that', '?']),\n",
       "       list(['yes', '!', 'i', 'just', 'caught', 'that', '!', 'i', 'have', 'edited', 'some', 'examples', 'that', 'i', 'am', 'really', 'confused', 'about', ',', 'where', 'x', 'is', 'not', 'necessarily', 'a', 'square', 'matrix', '.', 'thank', 'you', 'so', 'much', '!'])],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check 5 random token values\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Uqjl-9BxdAWm"
   },
   "source": [
    "We split the dataset into a training and a testing subset. \n",
    "\n",
    "The testing subset is composed of the titles, the train subset is composed of posts and comments\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VYlDBQtadOfU"
   },
   "outputs": [],
   "source": [
    "#Create train and test dataframes\n",
    "#Training df is all posts and comments and testing df is all titles\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 799
    },
    "colab_type": "code",
    "id": "lN7UD6Baqr4E",
    "outputId": "ca897b4f-4f07-4a3c-9782-bfeba7026ee2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- Training set: (705964, 7)\n",
      "\n",
      "   post_id  parent_id  comment_id  \\\n",
      "0   161009        NaN    309845.0   \n",
      "1   156252        NaN    298634.0   \n",
      "2   423360        NaN    790161.0   \n",
      "3   268623        NaN         NaN   \n",
      "4   433662        NaN    808873.0   \n",
      "\n",
      "                                                text category  \\\n",
      "0  I can't disclose the algorithm, but I can cert...  comment   \n",
      "1  I plan to leave the answer to this question in...  comment   \n",
      "2  Wait, I need to clarify how is Half-normal dis...  comment   \n",
      "3  I am fitting several models of the form.. glm ...     post   \n",
      "4  If you really want to calculate some p-value u...  comment   \n",
      "\n",
      "                                              tokens  n_tokens  \n",
      "0  [i, can, ', t, disclose, the, algorithm, ,, bu...        40  \n",
      "1  [i, plan, to, leave, the, answer, to, this, qu...        84  \n",
      "2  [wait, ,, i, need, to, clarify, how, is, half,...        25  \n",
      "3  [i, am, fitting, several, models, of, the, for...        82  \n",
      "4  [if, you, really, want, to, calculate, some, p...        66  \n",
      "\n",
      "-- Testing set (83685, 7)\n",
      "\n",
      "    post_id  parent_id  comment_id  \\\n",
      "21   154700        NaN         NaN   \n",
      "24   160640        NaN         NaN   \n",
      "27   148203        NaN         NaN   \n",
      "28   327174        NaN         NaN   \n",
      "33   169986        NaN         NaN   \n",
      "\n",
      "                                                 text category  \\\n",
      "21  Are aov with Error same as lmer of lme package...    title   \n",
      "24  How to compare contingency tables for a specif...    title   \n",
      "27        One-sided significance test for correlation    title   \n",
      "28  Visualization activization maximization for re...    title   \n",
      "33  Meaning of Intercept and what the intercept sh...    title   \n",
      "\n",
      "                                               tokens  n_tokens  \n",
      "21  [are, aov, with, error, same, as, lmer, of, lm...        13  \n",
      "24  [how, to, compare, contingency, tables, for, a...        10  \n",
      "27  [one, -, sided, significance, test, for, corre...         7  \n",
      "28  [visualization, activization, maximization, fo...         8  \n",
      "33  [meaning, of, intercept, and, what, the, inter...        14  \n"
     ]
    }
   ],
   "source": [
    "# Display the dimensions of the training dataframe \n",
    "\n",
    "# and the 1st 5 lines\n",
    "\n",
    "# Display the dimensions of the testing dataframe \n",
    "\n",
    "# and the 1st 5 lines\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oegvQAzqyyST"
   },
   "source": [
    "# Counting bigrams and following tokens\n",
    "We build a counts object defined as a defaultdict(Counter). \n",
    "\n",
    "Taking into account all trigrams (ngrams_degree = 3) that we break into prefix (bigrams) followed by single tokens. \n",
    "\n",
    "The counts object will have the bigrams as keys and for each key a Counter of all the potential tokens. \n",
    "\n",
    "For instance, if the corpus contains a 100 instances of \"*how many people*\" and a 120 instances of \"*how many times*\" we would get the following entry:\n",
    "\n",
    "    counts[('how', 'many')] = Counter('people': 100, 'times': 120, .... )\n",
    "\n",
    "Similarly if the corpus contains \"*the model is*\" 500 times and \"*the model parameters*\" 200 times, we end up with:\n",
    "\n",
    "    counts[('the', 'model')] = Counter('is': 500, 'parameters': 200, .... )\n",
    "\n",
    "To split the tokens into bigramns we use the [ntlk.ngrams](https://www.nltk.org/api/nltk.html#nltk.util.ngrams) function:\n",
    "\n",
    "\n",
    "    Return the ngrams generated from a sequence of items, as an iterator.\n",
    "    For example:\n",
    "\n",
    "    >>> from nltk.util import ngrams\n",
    "    >>> list(ngrams([1,2,3,4,5], 3))\n",
    "    [(1, 2, 3), (2, 3, 4), (3, 4, 5)]\n",
    "\n",
    "The next cell should take a couple of minutes.\n",
    "\n",
    "Note that we build the mode on the training subset df_train and leave the testing subset aside.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "L8cvgLCysPZR",
    "outputId": "ed36d02c-4bee-41bb-d7da-ac9a01f5aa14"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 705964/705964 [05:54<00:00, 1989.42it/s] \n"
     ]
    }
   ],
   "source": [
    "#Create a counts object by creating a defaultdict with a counter \n",
    "#This will be dictionary with a bigram for each key and a counter of potential tokens for each value\n",
    "\n",
    "#Loop through all the tokens in training set\n",
    "    \n",
    "    #For each ngram\n",
    "\n",
    "    \n",
    "          #Set n = ngrams_degree = 3 in our case\n",
    "\n",
    "          #Set pad right to true \n",
    "          \n",
    "          #Set pad right to true\n",
    "\n",
    "          #Set the left_pad_symbol = <s> \n",
    "\n",
    "          #Set right_pad_symbol = </s> \n",
    "\n",
    "            \n",
    "        #Get the prefix bigram (beginning to 2nd last index)\n",
    "\n",
    "        #Get the following token (last index)\n",
    "\n",
    "        #Add 1 to the counts object with prefix as the key and token as the value\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "NIfMPB3H1PZF"
   },
   "source": [
    "We can explore the counts object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "TbMNhbnCsb3b",
    "outputId": "4c754da2-edbe-46a3-88be-bc5fef41d91e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 3332935 bigrams\n"
     ]
    }
   ],
   "source": [
    "#Check the number of bigrams\n",
    "print(\"we have {} bigrams\".format(len(counts.keys())))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "UZAt_buMscsL",
    "outputId": "b49bdc8c-ebf6-48b5-a8d8-856e8755a8dc"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('np', 'a'): \tCounter({'b': 3, ',': 1, 'p': 1, '-': 1, 'np': 1, '…': 1, 's': 1})\n",
      "('fragments', 'regarding'): \tCounter({'the': 1})\n",
      "('this', 'taske'): \tCounter({'while': 1})\n",
      "('on', 'adjr'): \tCounter({'not': 1})\n",
      "('possible', 'accuracy'): \tCounter({'.': 3, 'across': 1})\n"
     ]
    }
   ],
   "source": [
    "#Print 5 random samples from counts object\n",
    "for i in range(5):\n",
    "    prefix = random.choice(list(counts.keys()))\n",
    "    print(\"{}: \\t{}\".format(prefix,counts[prefix]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "5coZ8qANhBmn"
   },
   "source": [
    "Let's look at the number of potential tokens for each bigram. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "QKHGK16sicW_"
   },
   "outputs": [],
   "source": [
    "#Loop through bigrams and get length of potential tokens \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 374
    },
    "colab_type": "code",
    "id": "z36-SPk0hLqe",
    "outputId": "d1a108a7-4fae-458d-eac3-836cc5472357"
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x432 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot the distribution of the count of tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mgWVBhFOjTtN"
   },
   "source": [
    "As we can see, most bigrams only have one potential following token. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "QUI1f7ChjXDT",
    "outputId": "9e8848bd-eba3-4aeb-b88c-48de227f35fb"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2146242 bigrams_with_single_tokens\n",
      "446829 bigrams_with_two_tokens\n"
     ]
    }
   ],
   "source": [
    "#Filter bigrams with exactly 1 potential following token\n",
    "\n",
    "#Filter bigramswith exactly 2 potential following tokens\n",
    "\n",
    "\n",
    "#Print the length of these filtered lists\n",
    "#Number of bigrams with exactly 1 potential following token\n",
    "\n",
    "#Number of bigrams with exactly 2 potential following token\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "66Oqu0fni4xo"
   },
   "source": [
    "2 bigrams have over 10,000 potential tokens:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "maq32UCnhCVf",
    "outputId": "bf23737a-fa01-41a0-d53e-3cb0e1ffc952"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{('<s>', '<s>'): 11564, ('of', 'the'): 10815}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check any bigrams that have more than 10,000 potential tokens\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gs4sLf7ZkgOU"
   },
   "source": [
    "Note: At this point we could decide to remove all the bigrams with a single potential token as not being significant. This would reduce the size of the model. We will see later on if that actually improves the model or degrades it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "a717NY_i3cyK"
   },
   "source": [
    "# token / prefix probabilities\n",
    "\n",
    "To obtain token / prefix probabilities using the Maximum Likelihood Estimator, we must simply normalize each (prefix - token) count by the total number of the prefix occurence. \n",
    "\n",
    "$$p(token / prefix) = \\frac{count(prefix + token)} {count(prefix)}$$\n",
    "\n",
    "\n",
    "Keeping the same defaultdict(Counter) structure for the freq object, we should obtain something similar to \n",
    "\n",
    "\n",
    "    freq[('how', 'many')] = {'people': 0.14, 'times': 120, .... }\n",
    "\n",
    "with \n",
    "* p(people / how many) = c('how many people') / c('how many') \n",
    "* p(times / how many) = c('how many times') / c('how many')\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Lt1QYcHlssWI"
   },
   "outputs": [],
   "source": [
    "#Create a freq object by creating a defaultdict with a dictionary \n",
    "#This will be dictionary with a bigram for each key and a dictionary of following token probabilities for each value\n",
    "\n",
    "\n",
    "#Loop through prefix and tokens in counts\n",
    "\n",
    "\n",
    "    #Get the total number of potential following tokens\n",
    "\n",
    "    #Loop through all the potential following tokens  \n",
    "\n",
    "    \n",
    "        #Calculate token probability as count/total\n",
    "        #Update freq onbject with prefix as the key and token probability as the value\n",
    "\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DdpOwLnC4sVu"
   },
   "source": [
    "Which gives us the following sample:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "QsSnzS8x4mMe",
    "outputId": "66a3ab1b-3bf2-4221-c89d-da5725a52ec2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('works', 'great'): \t{'.': 0.32558139534883723, '!': 0.05813953488372093, 'on': 0.06976744186046512, 'for': 0.16279069767441862, ',': 0.1511627906976744, 'with': 0.06976744186046512, 'provided': 0.011627906976744186, '-': 0.011627906976744186, '</s>': 0.011627906976744186, 'only': 0.011627906976744186, 'if': 0.023255813953488372, 'less': 0.011627906976744186, 'and': 0.03488372093023256, 'glmnet': 0.011627906976744186, 'but': 0.011627906976744186, 'out': 0.023255813953488372}\n",
      "('appended', 'bit'): \t{'was': 1.0}\n",
      "('hiv', 'infection'): \t{'rate': 0.5, '.': 0.16666666666666666, 'by': 0.16666666666666666, 'or': 0.16666666666666666}\n",
      "('assumption', 'mlr'): \t{'linearity': 1.0}\n",
      "('added', 'uncorrelated'): \t{'controls': 1.0}\n"
     ]
    }
   ],
   "source": [
    "#Print 5 random samples from freq object\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "70ZlWvA54_JF"
   },
   "source": [
    "# Text generation\n",
    "\n",
    "Next we write a text generating function which:\n",
    "* takes a bigram as input\n",
    "  * Note: In this version of the function, the bigram must exist in the corpus\n",
    "* generates a new token by sampling the available tokens related to the bigram using the freq object as distribution \n",
    "* sliding the bigram to include the new token\n",
    "* generating a new token based on the new bigram\n",
    "* stopping when the text is N tokens long or the latest token is the end of string symbol\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "OGlrLVja4zC9"
   },
   "outputs": [],
   "source": [
    "#Define a generate function that takes in an input string and a integer n_words for the length of the generated text\n",
    "\n",
    "\n",
    "    #For the specified length\n",
    "    \n",
    "        #Get the prefix\n",
    "\n",
    "        # no available text\n",
    "\n",
    "        \n",
    "            \n",
    "        #Get all the potential tokens for this prefix as a list \n",
    "        #Since we stored the values also as a dictionary, this will just be the keys\n",
    "\n",
    "        #Get all the corresponding probabilities for the potential tokens as a list\n",
    "        #Since we stored the values also as a dictionary, this will just be the values\n",
    "\n",
    "        #Randomly choose from the candidates, using the probabilities to define the distribution \n",
    "        #(Using numpy random.choice)\n",
    "\n",
    "        \n",
    "            \n",
    "    #Return chosen token\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SYqAncrc6K0R"
   },
   "source": [
    "Now let's have some fun with that language model.\n",
    "\n",
    "You can choose any seed bigramns as long as it is present in the corpus. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 141
    },
    "colab_type": "code",
    "id": "_JH3qRGG6Gjb",
    "outputId": "4c8cbd56-d2e4-4951-a6d0-87b296c4db42"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "the model again , i need to aggregate and average across them ? </s>\n",
      "\n",
      "that distribution . y , y bias ... also i have problem . given a data set however in that population ? </s>\n",
      "\n",
      "to determine seasonality from data , calculating the confidence interval does not specify , one way to extract them from the statistics principles and practice of spatial correlations . </s>\n"
     ]
    }
   ],
   "source": [
    "#Test out some different input texts and see what our function generates. The input must be present in the original \n",
    "#corpus for the function to work...we will learn how to deal with unseen words in the next milestone!\n",
    "\n",
    "#Try the following inputs - 'the model', 'that distribution', 'to determine'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "fyqQm1ZP64T_"
   },
   "source": [
    "# Temperature sampling\n",
    "\n",
    "As you may have noticed, for some bigrams, one particular token may be much more frequent than the others potential tokens. \n",
    "\n",
    "For instance:\n",
    "\n",
    "* ('building', 'machine'): \t{'learning': 0.875, 'classification': 0.125}\n",
    "\n",
    "when generating the next token based on the bigram \"*building machine*\", most of the times the word \"learning\" will be chosen instead of \"classification\".\n",
    "\n",
    "In order to compensate these imbalances and improve the chances of less frequent tokens to be chosen we can sample with temperature.\n",
    "\n",
    "In order to increase the randomness of the next token selection given a prefix, we can flatten the distribution using the temperature $$\\tau$$ to define a new probability distribution as such:\n",
    "\n",
    "$$f_{\\tau}(p_i) = \\frac{ p_i^{\\frac{1}{\\tau}} }{ \\sum_j p_j^{\\frac{1}{\\tau}} }$$\n",
    "\n",
    "See [this post](https://stats.stackexchange.com/questions/255223/the-effect-of-temperature-in-temperature-sampling) for a more in-depth explanation on temperature sampling.\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Copy over the generate function - we only need to modify the probabilites using the temperature definition\n",
    "#New probabilites for each token. The rest of the function will be the same\n",
    "\n",
    "#Define a generate function that takes in an input string, a temperature value \n",
    "#and a integer n_words for the length of the generated text\n",
    "    \n",
    "    #For the specified length\n",
    "\n",
    "        #Get the prefix\n",
    "        \n",
    "        # no available text\n",
    "\n",
    "        \n",
    "            \n",
    "        #Get all the potential tokens for this prefix as a list \n",
    "        #Since we stored the values also as a dictionary, this will just be the keys\n",
    "\n",
    "        #Get all the corresponding probabilities for the potential tokens as a list\n",
    "        #Since we stored the values also as a dictionary, this will just be the values\n",
    "\n",
    "        \n",
    "        #Modify distribution using temperature defintion\n",
    "        #Calucluate denominator as the sum of each probability to the power of the input temperature\n",
    "\n",
    "        #Calculate new probabilities as (probability to the power of temperatature)/denominator for each probability\n",
    "\n",
    "        \n",
    "        #Randomly choose from the candidates, using the probabilities to define the distribution \n",
    "        #(Using numpy random.choice)\n",
    "\n",
    "        \n",
    "            \n",
    "    #Return chosen token\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "mWhmCw3s963C"
   },
   "source": [
    "Let's generate some text with different values for the temperature.\n",
    "\n",
    "The higher the temperature, the less chaotic (and shorter) the generated text will end up being."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 191
    },
    "colab_type": "code",
    "id": "nBUTOMX09zIu",
    "outputId": "ce0ecfca-7022-4fef-c87c-6af89ddbb65a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.01\n",
      "the model receives the second cumulant . interestingly this doesn ´ t the stark contrast in distances . however patients is pretty convincing to sample m , transition proportion , these three\n",
      "0.5\n",
      "the model conclusions will be notable . </s>\n",
      "1\n",
      "the model and data like text if word in each week , one card . if you standardize your variables out of the class probablity in the output from the mixing parameter\n",
      "3\n",
      "the model , and then see if there is no need to be a good fit for a given sample size , and i am not sure if i get a good\n",
      "10\n",
      "the model . </s>\n"
     ]
    }
   ],
   "source": [
    "#Test out some different temperatures (try 0.01, 0.5, 1, 3, 10) for the following input - 'the model',\n",
    "#See how the generated text changes with the temperature\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export data and model\n",
    "As in Milestone 1 we will export our test dataframe as csv after transforming the list of tokens into a space separated string.\n",
    "\n",
    "We will also have to export the freq and counts object to use in the next milestone. For these objects, we will have to export them as pickled files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write test dataframe to output csv\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#Save freq object as pickled file\n",
    "\n",
    "\n",
    "\n",
    "#Save counts object as pickled file\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
