{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JXuab6JpneJy"
   },
   "source": [
    "# Model Assessment and Out of Vocabulary\n",
    "In this milestone we will learn how to assess the quality of our n-gram langauge model and handle words not present in the original corpus. You can access the code for Milestone 2 here:[]\n",
    "\n",
    "In milestone 3, we will work on the following items:\n",
    "\n",
    "* Assessing the relevance of a sentence by calcultaing its perplexity  \n",
    "* Handling n-grams that are not present in the corpus with Laplace smoothing\n",
    "\n",
    "The goal is to assess the quality of our n-gram langauge model and be able to generate new words/text outside of the original corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pickle\n",
    "import random\n",
    "from tqdm import tqdm\n",
    "from collections import defaultdict, Counter\n",
    "from nltk.util import ngrams\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if there's a problem with the versions of the librairies, you can . . uncomment this line and install the proper versions\n",
    "\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set some global parameters\n",
    "\n",
    "# Displaying all columns when displaying dataframes\n",
    "pd.options.display.max_columns = None\n",
    "\n",
    "# We will work with trigrams \n",
    "ngrams_degree = 3\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load test df to assess quality of our n-gram language model\n",
    "df_test = pd.read_csv('stackexchange_812k_tokenized_test.csv').reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>tokens</th>\n",
       "      <th>n_tokens</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>154700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Are aov with Error same as lmer of lme package...</td>\n",
       "      <td>title</td>\n",
       "      <td>['are', 'aov', 'with', 'error', 'same', 'as', ...</td>\n",
       "      <td>13</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160640</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How to compare contingency tables for a specif...</td>\n",
       "      <td>title</td>\n",
       "      <td>['how', 'to', 'compare', 'contingency', 'table...</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>148203</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>One-sided significance test for correlation</td>\n",
       "      <td>title</td>\n",
       "      <td>['one', '-', 'sided', 'significance', 'test', ...</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>327174</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Visualization activization maximization for re...</td>\n",
       "      <td>title</td>\n",
       "      <td>['visualization', 'activization', 'maximizatio...</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>169986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Meaning of Intercept and what the intercept sh...</td>\n",
       "      <td>title</td>\n",
       "      <td>['meaning', 'of', 'intercept', 'and', 'what', ...</td>\n",
       "      <td>14</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id  parent_id  comment_id  \\\n",
       "0   154700        NaN         NaN   \n",
       "1   160640        NaN         NaN   \n",
       "2   148203        NaN         NaN   \n",
       "3   327174        NaN         NaN   \n",
       "4   169986        NaN         NaN   \n",
       "\n",
       "                                                text category  \\\n",
       "0  Are aov with Error same as lmer of lme package...    title   \n",
       "1  How to compare contingency tables for a specif...    title   \n",
       "2        One-sided significance test for correlation    title   \n",
       "3  Visualization activization maximization for re...    title   \n",
       "4  Meaning of Intercept and what the intercept sh...    title   \n",
       "\n",
       "                                              tokens  n_tokens  \n",
       "0  ['are', 'aov', 'with', 'error', 'same', 'as', ...        13  \n",
       "1  ['how', 'to', 'compare', 'contingency', 'table...        10  \n",
       "2  ['one', '-', 'sided', 'significance', 'test', ...         7  \n",
       "3  ['visualization', 'activization', 'maximizatio...         8  \n",
       "4  ['meaning', 'of', 'intercept', 'and', 'what', ...        14  "
      ]
     },
     "execution_count": 122,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check the 1st 5 lines\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Load counts and freq object from Milestone 2\n",
    "with open('bigram_freq.pickle', 'rb') as handle:\n",
    "    freq = pickle.load(handle)\n",
    "with open('bigram_counts.pickle', 'rb') as handle:\n",
    "    counts = pickle.load(handle)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('observability', 'matrix'): \tCounter({'is': 3})\n",
      "('ve', 'starting'): \tCounter({'to': 1})\n",
      "('durch', 'dieses'): \tCounter({'verfahren': 1})\n",
      "('as', 'jack'): \tCounter({'so': 1, 'tanner': 1})\n",
      "('logest', 'function'): \tCounter({'.': 1, 'in': 1})\n"
     ]
    }
   ],
   "source": [
    "#Print 5 random samples from counts object to check\n",
    "for i in range(5):\n",
    "    prefix = random.choice(list(counts.keys()))\n",
    "    print(\"{}: \\t{}\".format(prefix,counts[prefix]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('factor', 'yr'): \t{'factor': 0.05263157894736842, '.': 0.5789473684210527, '-': 0.15789473684210525, ',': 0.10526315789473684, 'model': 0.05263157894736842, 'anova': 0.05263157894736842}\n",
      "('me', 'betas'): \t{'equal': 1.0}\n",
      "('model', 'predicting'): \t{'survival': 0.011111111111111112, 'recidivism': 0.011111111111111112, 'the': 0.25555555555555554, 'this': 0.022222222222222223, 'if': 0.011111111111111112, 'negative': 0.011111111111111112, 'z': 0.011111111111111112, 'sports': 0.011111111111111112, 'response': 0.011111111111111112, 'child': 0.011111111111111112, 'a': 0.06666666666666667, 'y': 0.022222222222222223, 'mileage': 0.044444444444444446, 'species': 0.011111111111111112, 'pay': 0.011111111111111112, 'just': 0.022222222222222223, 'price': 0.011111111111111112, '?': 0.011111111111111112, 'current': 0.011111111111111112, 'conversion': 0.011111111111111112, 'an': 0.011111111111111112, 'symptoms': 0.011111111111111112, 'whether': 0.03333333333333333, 'better': 0.011111111111111112, 'cnt': 0.011111111111111112, 'observed': 0.011111111111111112, 'so': 0.011111111111111112, 'customer': 0.022222222222222223, 'clickrate': 0.011111111111111112, 'how': 0.022222222222222223, 'absorbance': 0.011111111111111112, 'step': 0.011111111111111112, 'ρ': 0.011111111111111112, 'one': 0.022222222222222223, 'count': 0.011111111111111112, 'angles': 0.011111111111111112, 'grades': 0.011111111111111112, 'correctly': 0.022222222222222223, 'what': 0.011111111111111112, 'weight': 0.011111111111111112, 'rt': 0.011111111111111112, \"'\": 0.011111111111111112, 'apple': 0.011111111111111112, 'of': 0.011111111111111112, 'property': 0.011111111111111112, ',': 0.011111111111111112, 'everything': 0.011111111111111112, 'class': 0.011111111111111112, 'nba': 0.011111111111111112, '.': 0.011111111111111112, 'incorrectly': 0.011111111111111112}\n",
      "('go', 'retention'): \t{'rates': 1.0}\n",
      "('⁷th', 'between'): \t{'the': 1.0}\n"
     ]
    }
   ],
   "source": [
    "#Print 5 random samples from freq object to check\n",
    "for i in range(5):\n",
    "    prefix = random.choice(list(freq.keys()))\n",
    "    print(\"{}: \\t{}\".format(prefix,freq[prefix]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "jQ4OxyWU_RTG"
   },
   "source": [
    "# Perplexity\n",
    "\n",
    "Let's now implement a way to measure the quality of our model.\n",
    "\n",
    "The idea is to estimate the probability of a test sentence given our model. \n",
    "An uncommon sentence should be less probable than a common one.\n",
    "\n",
    "\n",
    "Notes : \n",
    "  1. At this point the sentence should exist in the corpus. Our model does not know yet how to handle out-of-vocabulary (OOV) bigrams, trigrams or tokens.\n",
    "  2. To avoid the problem of underflow caused by multiplying multiple very small floats, we work in the log space:\n",
    "\n",
    "So instead of calculating perplexity with (case ngrams_degree = 3):\n",
    " \n",
    "$$PP(w_{1},\\cdots, w_N) = ( \\prod_{i = 3}^{N} \\frac{1}{ p(w_i/ w_{i-2}w_{i-1} )} )^{\\frac{1}{N}}$$\n",
    "\n",
    "We compute\n",
    "\n",
    "$$PP(w_{1},\\cdots, w_N) = \\exp [ - \\frac{1}{N} {\\sum_{i = 3}^{N} \\log {p(w_i/ w_{i-2}w_{i-1}} } ) ]$$\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sBaXy5fJ-SgY"
   },
   "outputs": [],
   "source": [
    "#Define a tokenizer object using WordPunctTokenizer from NLTK\n",
    "tokenizer = WordPunctTokenizer()\n",
    "\n",
    "#Define a generate function that takes in an input sentence and returns the perplexity score of that sentence\n",
    "\n",
    "def perplexity(sentence):\n",
    "    \n",
    "    #Convert input sentence to lowercase and tokenize\n",
    "    sentence = tokenizer.tokenize(sentence.lower())\n",
    "    #Get number of tokens\n",
    "    N = len(sentence)\n",
    "    \n",
    "    #Initialize logprob to be 0 - we will add the log probabilities of each ngram to this variable\n",
    "    #and then take the exponent at the end to calculate the perplexity\n",
    "    logprob = 0\n",
    "    \n",
    "    #For each ngram in the sentence\n",
    "    for ngram in ngrams(\n",
    "          sentence, \n",
    "          n= ngrams_degree,  \n",
    "          pad_right=True, pad_left=True, \n",
    "          left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\"):\n",
    "        \n",
    "        #Try except block in case the prefix/token doesn't exist in our model\n",
    "        try:\n",
    "            #Get the prefix bigram (beginning to 2nd last index)\n",
    "            prefix = ngram[:ngrams_degree-1] \n",
    "            #Get the following token (last index)\n",
    "            token = ngram[ngrams_degree-1]\n",
    "            #Get the corresponding probability of that prefix/token combination from the freq object\n",
    "            #and calulate the log of this probability. Add this value to the logprob variable.\n",
    "            logprob += np.log( freq[ prefix ][token]  )\n",
    "        except:\n",
    "            #Pass in case prefix/token doesn't exist in freq object\n",
    "            pass\n",
    "    \n",
    "    #Return the perpexity - calculate using previous definition\n",
    "    #Take the exponent of -(sum of logprobabilities)/number of tokens \n",
    "    return np.exp(- logprob / N)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "HvFOsEsxAFlI"
   },
   "source": [
    "Let's calculate the perplexity on some sentences.\n",
    "\n",
    "Take the time to see how the perplexity score varies when you . . modify the sentence. For instance compare the perplexity for\n",
    "\n",
    "* *the difference between the two approaches is discussed here.*\n",
    "* *the difference between the two approaches is discussed here*\n",
    "* *the difference between the two approaches*\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 104
    },
    "colab_type": "code",
    "id": "fASAO_JzAEbc",
    "outputId": "09abf878-8996-44d3-c5c6-cfaa2aa6c1c7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perplexity 22.63] the difference between the two approaches is discussed here\n",
      "\n",
      "[perplexity 38.12] this question really belongs on a different site\n",
      "\n",
      "[perplexity 72.18] The function may only be linear in the region where the points were taken\n"
     ]
    }
   ],
   "source": [
    "#Calculate the perplexity of some test sentences\n",
    "sentence = \"the difference between the two approaches is discussed here\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity(sentence), sentence))\n",
    "\n",
    "sentence = \"this question really belongs on a different site\"\n",
    "print()\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity(sentence), sentence))\n",
    "\n",
    "sentence = \"The function may only be linear in the region where the points were taken\"\n",
    "print()\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity(sentence), sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "ekhHxEZOC0Ht"
   },
   "source": [
    "# Out of Vocabulary (OOV) \n",
    "\n",
    "The main weakness of our model so far is that it does not know how to handle elements that are not already in the original corpus.\n",
    "\n",
    "Since both when generating text and when calculating perplexity we use the count of the prefix in the corpus, when that prefix is missing, the counts = 0  which causes problems with logs and divisions.\n",
    "\n",
    "To remediate to that problem we can artificially assign a probability (although a very low one) to missing ngrams and tokens.\n",
    "\n",
    "This method is called Laplace smoothing. It relies on calculating the frequency of a token / prefix with:\n",
    "\n",
    "$$ p(token / prefix) = \\frac{ count( prefix + token) + \\delta}{count(prefix) + \\delta \\times |N| }$$\n",
    "\n",
    "\n",
    "Where \n",
    "\n",
    "* N is the total number of prefixes in the model\n",
    "* delta is an arbitrary number \n",
    "\n",
    "When the prefix is missing from the original corpus, the probability of a token / prefix will now be:\n",
    "\n",
    "$$p(token / prefix) = \\frac{1} { | N |}$$\n",
    "\n",
    "Let's implement that perplexity with Laplace Smoothing\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "1Lbwk0_rqrXa"
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "bMPyoR4xALs2"
   },
   "outputs": [],
   "source": [
    "#We can modify our original perplexity function to deal with words not in our model.\n",
    "#In the original function we simply skipped calculating the probabilities for any prefix/tokens\n",
    "#that didn't have probabilities. In this function, rather than skipping these cases we will \n",
    "#implement Laplace smoothing - artificially adding a small probability to this missing tokens and prefixes.\n",
    "\n",
    "#Define a generate function that takes in an input sentence and returns the perplexity score of that sentence\n",
    "#With addititve laplace smoothing for any words that are not in the original corpus.\n",
    "\n",
    "def perplexity_laplace(sentence,delta = 1):\n",
    "    \n",
    "    #Convert input sentence to lowercase and tokenize\n",
    "    sentence = tokenizer.tokenize(sentence.lower())\n",
    "    #Get number of tokens\n",
    "    N = len(sentence)\n",
    "    \n",
    "    #Initialize logprob to be 0 - we will add the log probabilities of each ngram to this variable\n",
    "    #and then take the exponent at the end to calculate the perplexity\n",
    "    logprob = 0\n",
    "    \n",
    "    #For each ngram in the sentence\n",
    "    for ngram in ngrams(\n",
    "          sentence, \n",
    "          n= ngrams_degree,  \n",
    "          pad_right=True, pad_left=True, \n",
    "          left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\"):\n",
    "        \n",
    "        #Get the prefix bigram (beginning to 2nd last index)\n",
    "        prefix = ngram[:ngrams_degree-1]\n",
    "        #Get the following token (last index)\n",
    "        token = ngram[ngrams_degree-1]\n",
    "        \n",
    "        #If prefix is present in model\n",
    "        if prefix in list(counts.keys()):\n",
    "            #Get the combined count of the potential following tokens\n",
    "            total = sum( counts[prefix].values()  )\n",
    "            #If following token is present in model\n",
    "            if token in counts[prefix].keys():\n",
    "                #Get the corresponding probability of that prefix/token combination from the counts object\n",
    "                #and implement Laplace smoothing using the formula defined above.\n",
    "                #We need to modify our prefix/token probability calculation by adding delta to the\n",
    "                #numerator and delta*number of tokens to the denominator. This adds an artifical probability \n",
    "                #so we can still return a probability in the case where the counts are 0.\n",
    "                #As before take the log of this value and add it to the logprob variable.\n",
    "                logprob += np.log( (counts[prefix][token] + delta)/ (total + delta * N ) )\n",
    "            else:\n",
    "                #If following token is missing then calculate using Laplace smoothing with delta\n",
    "                logprob += np.log( ( delta)/ (total + delta * N ) )\n",
    "        else:\n",
    "            #If prefix is missing then simply calculate as log of number of tokens\n",
    "            logprob += - np.log( N )\n",
    "  \n",
    "    #Return the perpexity - calculate using previous definition\n",
    "    #Take the exponent of -(sum of logprobabilities)/number of tokens \n",
    "    return np.exp(-logprob / N)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "argLcwydEbS2"
   },
   "source": [
    "We can now calculate the perplexity of sentences that were not present in the original corpus. \n",
    "\n",
    "For instance: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "kXQ0559WAv27",
    "outputId": "4c028db3-7011-41c3-8ba6-b2e00e9cfeee"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[perplexity 142.66] this model belongs on a different planet\n",
      "[perplexity 35.50] this question really belongs on a different site.\n"
     ]
    }
   ],
   "source": [
    "#Calculate the perplexity of some new sentences - you can make them up!\n",
    "#Try with different values for delta and see how the perpelexity changes, especially for made-up sentences\n",
    "\n",
    "#Try the following 2 sentences- \"this model belongs on a different planet\", \"this question really belongs on a different site.\"\n",
    "#Try out a delta value of 1\n",
    "\n",
    "sentence = \"this model belongs on a different planet\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity_laplace(sentence, delta = 10), sentence))\n",
    "\n",
    "sentence = \"this question really belongs on a different site.\"\n",
    "print(\"[perplexity {:.2f}] {}\".format(perplexity_laplace(sentence, delta = 10), sentence))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 86
    },
    "colab_type": "code",
    "id": "m6uDCIVAAwqp",
    "outputId": "acb0fb1f-27b5-48de-ef67-c77531f6b209"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[perplexity 319.66] this model belongs on a different planet\n",
      "\n",
      "[perplexity 36.10] this question really belongs on a different site.\n"
     ]
    }
   ],
   "source": [
    "#Try the following 2 sentences- \"this model belongs on a different planet\", \"this question really belongs on a different site.\"\n",
    "#Try out a delta value of 10\n",
    "\n",
    "sentence = \"this model belongs on a different planet\"\n",
    "print(\"\\n[perplexity {:.2f}] {}\".format(perplexity_laplace(sentence, delta = 1), sentence))\n",
    "\n",
    "sentence = \"this question really belongs on a different site.\"\n",
    "print(\"\\n[perplexity {:.2f}] {}\".format(perplexity_laplace(sentence, delta = 1), sentence))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "k008Cdzt0VH5"
   },
   "source": [
    "# Perplexity on the test corpus and sentence probability\n",
    "\n",
    "How do we calculate the perplexity of a model on a test corpus.\n",
    "\n",
    "Let's say we have *m* sentences in the corpus, the perplexity of the corpus is given by \n",
    "\n",
    "$$ PP(Corpus) = P(S_1, \\cdots, S_m)^{-\\frac{1}{N}} $$\n",
    "\n",
    "We can assume that the sentences are independent\n",
    "\n",
    "$$ PP(Corpus) = (\\prod_{k = 1}^{m}  P(S_k))^{-\\frac{1}{N}} $$\n",
    "\n",
    "Which we calculate in the log space to avoid underflow\n",
    "\n",
    "$$ PP(Corpus) = \\exp ( -\\frac{1}{N} \\sum_{k = 1}^{m}  log(P(S_k)) $$\n",
    "\n",
    "So to calculate the perplexity on a test corpus we need to calculate the probability of each single sentence.\n",
    "\n",
    "The following function calculates the probability of a sentence. \n",
    "\n",
    "Instead of using laplace smoothing to deal with the missing bigrams and tokens, we will simply skip missing elements to make the function faster.\n",
    "Implementing laplace smoothing requires several extra conditions that are taking too much time to run.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S9DMcOnEIAcp"
   },
   "outputs": [],
   "source": [
    "#We can modify our original perplexity function to simply return the sum of the logprobabilities.\n",
    "\n",
    "#Define a generate function that takes in an input sentence and returns the sum of the \n",
    "#logprobabilities for the tokens in the sentence. \n",
    "def logproba_sentence(sentence, delta = 1):\n",
    "    #Convert input sentence to lowercase and tokenize\n",
    "    sentence = tokenizer.tokenize(sentence.lower())\n",
    "    #Get number of tokens\n",
    "    N = len(sentence)\n",
    "    \n",
    "    #Initialize logprob to be 0 - we will add the log probabilities of each ngram to this variable\n",
    "    #and then take the exponent at the end to calculate the perplexity\n",
    "    logprob = 0\n",
    "    \n",
    "    #For each ngram in the sentence\n",
    "    for ngram in ngrams(\n",
    "          sentence, \n",
    "          n= ngrams_degree,  \n",
    "          pad_right=True, pad_left=True, \n",
    "          left_pad_symbol=\"<s>\", right_pad_symbol=\"</s>\"):\n",
    "        \n",
    "        #Try except block in case the prefix/token doesn't exist in our model\n",
    "        try:\n",
    "            #Get the prefix bigram (beginning to 2nd last index)\n",
    "            prefix = ngram[:ngrams_degree-1] \n",
    "            #Get the following token (last index)\n",
    "            token = ngram[ngrams_degree-1]\n",
    "            #Get the corresponding probability of that prefix/token combination from the freq object\n",
    "            #and calulate the log of this probability. Add this value to the logprob variable.\n",
    "            logprob += np.log( freq[ prefix ][token]  )\n",
    "        except:\n",
    "            #Pass in case prefix/token doesn't exist in freq object\n",
    "            pass\n",
    "    \n",
    "    #Return the sum of logprobabilities\n",
    "    return logprob"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xv0NB4Qz3T0j"
   },
   "source": [
    "We can now implement the perplexity for a whole set of sentences\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "J2vZmwl5xajz"
   },
   "outputs": [],
   "source": [
    "#Define a function calculate the perplexity of an input corpus (list of sentences)\n",
    "def corpus_perplexity(corpus):\n",
    "    #Start by calculating the total number of tokens in the corpus\n",
    "    #Combine all the sentences together to form a single string\n",
    "    all_sentences = ' '.join(corpus)\n",
    "    #Convert combined sentence string to lowercase and tokenize\n",
    "    all_tokens =  tokenizer.tokenize(all_sentences.lower())\n",
    "    #Get number of tokens in combined sentence/corpus\n",
    "    N = len(all_tokens)\n",
    "    \n",
    "    #Initialize logprob to be 0 - we will add the log probabilities of each sentence to this variable\n",
    "    #and then take the exponent at the end to calculate the perplexity\n",
    "    logprob = 0\n",
    "    \n",
    "    #For each sentence in the input corpus\n",
    "    for sentence in tqdm(corpus):\n",
    "        #Calculate the logprobability of that sentence using our previously defined function.\n",
    "        #Add this value to the logprob variable.\n",
    "        logprob += logproba_sentence(sentence)\n",
    "    \n",
    "    #Return the corpus perpexity - calculate using definition as before\n",
    "    #Take the exponent of -(sum of logprobabilities of sentences)/number of tokens in corpus\n",
    "    return np.exp( - logprob / N)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate corpus perplexity\n",
    "Let's now calculate the perplexity of our test corpus composed of just the titles. First we'll calculate the perplexity of a random sample of 1000 sentences from the test set. Then we we'll try and calculate the perplexity of the entire test corpus as well - however there may be an overflow warning if the number is too big!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "d0actWrZxwHV",
    "outputId": "956f0690-c16f-47a4-ab90-5779d4692bc5"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 1000/1000 [00:06<00:00, 152.88it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "21.584069021216166\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate the perplexity of a sample of 1000 titles and save this value to a variable cp_1000\n",
    "corpus = df_test.text.sample(1000, random_state = 8).values\n",
    "cp_1000 = corpus_perplexity(corpus)\n",
    "#Print value of cp_1000\n",
    "print(cp_1000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 143,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 52
    },
    "colab_type": "code",
    "id": "VuYVQ3Kd0NV2",
    "outputId": "64dac499-1c2b-4b8d-9ea7-9fbcac66a087",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 83685/83685 [00:05<00:00, 14843.50it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "21.197697454704233"
      ]
     },
     "execution_count": 143,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Calculate the perplexity of the whole test corpus\n",
    "corpus_perplexity(df_test.text.values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "      <th>tokens</th>\n",
       "      <th>n_tokens</th>\n",
       "      <th>sample_perplexity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>154700</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Are aov with Error same as lmer of lme package...</td>\n",
       "      <td>title</td>\n",
       "      <td>['are', 'aov', 'with', 'error', 'same', 'as', ...</td>\n",
       "      <td>13</td>\n",
       "      <td>21.584069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160640</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>How to compare contingency tables for a specif...</td>\n",
       "      <td>title</td>\n",
       "      <td>['how', 'to', 'compare', 'contingency', 'table...</td>\n",
       "      <td>10</td>\n",
       "      <td>21.584069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>148203</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>One-sided significance test for correlation</td>\n",
       "      <td>title</td>\n",
       "      <td>['one', '-', 'sided', 'significance', 'test', ...</td>\n",
       "      <td>7</td>\n",
       "      <td>21.584069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>327174</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Visualization activization maximization for re...</td>\n",
       "      <td>title</td>\n",
       "      <td>['visualization', 'activization', 'maximizatio...</td>\n",
       "      <td>8</td>\n",
       "      <td>21.584069</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>169986</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Meaning of Intercept and what the intercept sh...</td>\n",
       "      <td>title</td>\n",
       "      <td>['meaning', 'of', 'intercept', 'and', 'what', ...</td>\n",
       "      <td>14</td>\n",
       "      <td>21.584069</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id  parent_id  comment_id  \\\n",
       "0   154700        NaN         NaN   \n",
       "1   160640        NaN         NaN   \n",
       "2   148203        NaN         NaN   \n",
       "3   327174        NaN         NaN   \n",
       "4   169986        NaN         NaN   \n",
       "\n",
       "                                                text category  \\\n",
       "0  Are aov with Error same as lmer of lme package...    title   \n",
       "1  How to compare contingency tables for a specif...    title   \n",
       "2        One-sided significance test for correlation    title   \n",
       "3  Visualization activization maximization for re...    title   \n",
       "4  Meaning of Intercept and what the intercept sh...    title   \n",
       "\n",
       "                                              tokens  n_tokens  \\\n",
       "0  ['are', 'aov', 'with', 'error', 'same', 'as', ...        13   \n",
       "1  ['how', 'to', 'compare', 'contingency', 'table...        10   \n",
       "2  ['one', '-', 'sided', 'significance', 'test', ...         7   \n",
       "3  ['visualization', 'activization', 'maximizatio...         8   \n",
       "4  ['meaning', 'of', 'intercept', 'and', 'what', ...        14   \n",
       "\n",
       "   sample_perplexity  \n",
       "0          21.584069  \n",
       "1          21.584069  \n",
       "2          21.584069  \n",
       "3          21.584069  \n",
       "4          21.584069  "
      ]
     },
     "execution_count": 146,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Create new column 'sample_perplexity' and set value to the sample perpelxity value we calculated\n",
    "df_test['sample_perplexity'] = cp_1000\n",
    "#Check first 5 lines\n",
    "df_test.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Export data and model\n",
    "As in Milestone 1 and 2 we will export our test dataframe as csv after transforming the list of tokens into a space separated string."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change tokens column to a space separated string of tokens rather a list\n",
    "data['tokens'] = data.tokens.apply(lambda tk : ' '.join(tk))\n",
    "\n",
    "#Write test dataframe to output csv\n",
    "df_test.to_csv('stackexchange_812k_tokenized_test.csv', quoting = csv.QUOTE_ALL, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
