{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "OeIHZdTXg03B"
   },
   "source": [
    "# Milestone 1: Clean and Tokenize Text\n",
    "The goal of this task is to to reduce the noise in the original raw text and tokenize the text to prepare it for the language maodel. We need to remove everything that is not exactly text (e.g html tags, math equations, urls, etc), filter out any rows with very short or very long texts and finally tokenize the text for use in a language model. The cleaned and tokenized output will be used as the input in the next milestone to build our n-gram model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PbiEgK7HS4L4"
   },
   "outputs": [],
   "source": [
    "# We only need the following librairies\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "import csv\n",
    "import numpy as np\n",
    "from nltk.tokenize import WordPunctTokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "yuV6ny4UvZqf"
   },
   "outputs": [],
   "source": [
    "# if there's a problem with the versions of the librairies, you can . . uncomment this line and install the proper versions\n",
    "\n",
    "# !pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "JfW0QBdig_k7"
   },
   "source": [
    "Let's load the dataset and shuffle it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "YWZx8tbrVrx8"
   },
   "outputs": [],
   "source": [
    "#Load data using pandas read_csv\n",
    "data = pd.read_csv('stackexchange_812k.csv').sample(frac = 1, random_state = 0).reset_index(drop = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JgzBFBwWvG3q"
   },
   "outputs": [],
   "source": [
    "#Check data has 812132 rows\n",
    "assert data.shape == (812132, 5), \"The dataset does not have the right dimensions\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WyWqejm3hUE-"
   },
   "source": [
    "And start by exploring the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "eiMeKSHbV7Lv",
    "outputId": "bb0bd4bd-6eb9-4c98-f1f7-1a47a8ecdff4"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>post_id</th>\n",
       "      <th>parent_id</th>\n",
       "      <th>comment_id</th>\n",
       "      <th>text</th>\n",
       "      <th>category</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>291254</td>\n",
       "      <td>NaN</td>\n",
       "      <td>601672.0</td>\n",
       "      <td>The condition makes the gradient unbiased. (it...</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>115372</td>\n",
       "      <td>NaN</td>\n",
       "      <td>221284.0</td>\n",
       "      <td>Yes, that sounds fine to me.</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>327356</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>&lt;p&gt;Consider gaussian variables belonging to a ...</td>\n",
       "      <td>post</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>186923</td>\n",
       "      <td>NaN</td>\n",
       "      <td>355055.0</td>\n",
       "      <td>Thanks S. Catterall. ^-^ Integrability: I knew...</td>\n",
       "      <td>comment</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>433143</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Feature with very few extreme values</td>\n",
       "      <td>title</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   post_id  parent_id  comment_id  \\\n",
       "0   291254        NaN    601672.0   \n",
       "1   115372        NaN    221284.0   \n",
       "2   327356        NaN         NaN   \n",
       "3   186923        NaN    355055.0   \n",
       "4   433143        NaN         NaN   \n",
       "\n",
       "                                                text category  \n",
       "0  The condition makes the gradient unbiased. (it...  comment  \n",
       "1                       Yes, that sounds fine to me.  comment  \n",
       "2  <p>Consider gaussian variables belonging to a ...     post  \n",
       "3  Thanks S. Catterall. ^-^ Integrability: I knew...  comment  \n",
       "4               Feature with very few extreme values    title  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the first few rows of the data\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Bon5CFezh_v1"
   },
   "source": [
    "We have 3 types of text:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "oQK2WV0JWADk",
    "outputId": "98f300dd-f186-4abb-b45c-ddaac4b59b43"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comment    553076\n",
       "post       167304\n",
       "title       91752\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Check the value counts for the category column\n",
    "data.category.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "vR-r4JgDiD5M",
    "outputId": "f08b65d9-05b0-499c-84a9-fafd45aae7e1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "comparison of coefficients (they are not Standardized Beta ones)\n",
      "--------------------\n",
      "Can data ever be too high dimensional for the Lasso?\n",
      "--------------------\n",
      "Which test should I use to predict a ratio from multilevel, time series data?\n"
     ]
    }
   ],
   "source": [
    "#Print a few examples of titles \n",
    "for p in data[data.category == 'title'].text.sample(3).values:\n",
    "    print('-' * 20)\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "iA47aMGKiSAk"
   },
   "source": [
    "We see that posts text have html tags and latex formatted equations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 658
    },
    "colab_type": "code",
    "id": "QagyZuf0iJqO",
    "outputId": "f7ecd5cf-8a19-4f5c-901e-e7a2ff847b49"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "<p>I'm looking for a suitable image dataset to train an SVM, a CNN and possibly an MLP as classifiers and to compare the results. Since an SVM archieves good results with small data sets and a CNN and above all an MLP requires a very long time for training with large datasets, this dataset should be rather smaller. But this search is quite difficult because the dataset should not be too big but not too small. The dataset should be suitable to train a CNN und possible even an MLP in the shortest possible time, I mean with GPU max. 1 day. It would probably be better if the image resolution is not too high. Since the features have to be extracted in advance in SVM, it would probably be better if the data is not too complex.</p>\n",
      "\n",
      "<p>Regarding CNN, I know that there are the so-called pre-trained networks, but I am not sure if they are really suitable for such a comparison.</p>\n",
      "\n",
      "<p>Does anyone have experience with it? I would be very grateful for any tip on a suitable dataset.</p>\n",
      "\n",
      "--------------------\n",
      "<p>To answer your question, the model is not totally useless as you will still get unbiased estimates for your linear regression parameters. However, the heteroscedasticity leads to inefficient estimates. In other words, the estimated variance of the estimates is too high and you should be cautious when making inferences from based on the standard errors. I cannot say more without knowing the objective of your estimation. </p>\n",
      "\n",
      "--------------------\n",
      "<p>I have sales data organised in a table with 6 columns (4 for the location and type data, and 2 for the dates and the quantity sold), and 24 rows for each category representing the sales over 24 months (there are 7104 rows, 24 for each of the 296 possibilities I have).</p>\n",
      "\n",
      "<p>I want to use <a href=\"http://robjhyndman.com/hyndsight/gts/\" rel=\"nofollow\">hts.gts</a>, but I need to create the group hierarchy and to change the format of the data for that. What would be the best way to do that in R?</p>\n",
      "\n",
      "<p>Additionally, is there a better way to store this data?</p>\n",
      "\n",
      "<p>EDIT: Here is how the data looks:</p>\n",
      "\n",
      "<pre><code>   PERIOD              PRODUCT.TYPE       BRAND      SIZE   CITY        VALUE\n",
      "1: 2012-01             A                  X          100    SOFIA       0.11134739\n",
      "2: 2012-02             A                  X          100    SOFIA       0.02486429\n",
      "3: 2012-03             A                  X          100    SOFIA       0.09738088\n",
      "4: 2012-04             A                  X          100    SOFIA       0.11401330\n",
      "5: 2012-05             A                  X          100    SOFIA       0.15660395\n",
      "6: 2012-06             A                  X          100    SOFIA       0.21496220\n",
      "</code></pre>\n",
      "\n",
      "<p>I have a bunch of different products types, each having a different amount of brands and sizes, and which are not all being sold in every city.</p>\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#Print a few examples of posts \n",
    "for p in data[data.category == 'post'].text.sample(3).values:\n",
    "    print('-' * 20)\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "No7lYssNiNCy",
    "outputId": "13568e3d-b64c-4d1f-904d-be913cbf2b2e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "I did also tutorials in R about Random Forest. The problem is not how to program, my problem is that I don't know what to program if the random forest has to be used. It is not about programming\n",
      "--------------------\n",
      "Never use regression with time series data.  Use a Transfer Function model approach.\n",
      "--------------------\n",
      "This is an awfully complex procedure. Let's step back for a minute.  What is the point of this?  What are you trying to do? Most likely we will not end up going this route.\n"
     ]
    }
   ],
   "source": [
    "#Print a few examples of comments \n",
    "for p in data[data.category == 'comment'].text.sample(3).values:\n",
    "    print('-' * 20)\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Mk-vHb7LiZrf"
   },
   "source": [
    "# Clean up raw text\n",
    "We're going to remove the following elements:\n",
    "* html tags\n",
    "* line returns\n",
    "* urls\n",
    "* latex equations\n",
    "* numbers\n",
    "* mentions: @someone\n",
    "* digits\n",
    "* most of the punctuation\n",
    "* and extra spaces\n",
    "\n",
    "For that we will use a series of simple regex patterns and the following pandas dataframe pattern:\n",
    "\n",
    "```\n",
    "pattern = r\" some regex pattern\"\n",
    "df.text.apply(lambda t : re.sub(pattern,' ', t) )\n",
    "```\n",
    "\n",
    "Note that it's up to you to decide which elements should be removed or kept. This sequence of transformations can be modified. \n",
    "\n",
    "Not also that the regex patterns we use here are chosen for their simplicity. Feel free to use more precise patterns.  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "VAr8qzoKiQjx"
   },
   "outputs": [],
   "source": [
    "# Remove html tags\n",
    "data['text'] = data.text.apply(lambda t : re.sub(\"<[^>]*>\",' ', t) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DS_VHPC1jN5i"
   },
   "outputs": [],
   "source": [
    "# Remove line returns\n",
    "data['text'] = data.text.apply(lambda t : re.sub(\"[\\r\\n]+\",' ', t) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TyQeTFN9jPvK"
   },
   "outputs": [],
   "source": [
    "# Remove urls\n",
    "data['text'] = data.text.apply(lambda t : re.sub(\"http\\S+\",' ', t) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "TI1vZWUFjRCT"
   },
   "outputs": [],
   "source": [
    "# Remove mentions\n",
    "data['text'] = data.text.apply(lambda t : re.sub(\"@\\S+\",' ', t) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "29nqngCpjST2"
   },
   "outputs": [],
   "source": [
    "# Remove latex\n",
    "data['text'] = data.text.apply(lambda t : re.sub(\"\\$[^>]*\\$\",' ', t) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "X2ZBG7PkjTlQ"
   },
   "outputs": [],
   "source": [
    "# Remove digits \n",
    "data['text'] = data.text.apply(lambda t : re.sub(\"\\d+\",' ', t) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "PEmEZVOwjVmy"
   },
   "outputs": [],
   "source": [
    "# Remove some of the punctuation but keep ,.!? and -\n",
    "remove = '\"#$%&()*+/:;<=>@[\\\\]^_`{|}~”“'\n",
    "pattern = r\"[{}]\".format(remove)\n",
    "data['text'] = data.text.apply(lambda t : re.sub(pattern,' ', t) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "DhtXc_dPjX88"
   },
   "outputs": [],
   "source": [
    "# Remove multiple spaces\n",
    "data['text'] = data.text.apply(lambda t : re.sub(\"\\s\\s+\",' ', t) )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "sYJyjInajZQG"
   },
   "outputs": [],
   "source": [
    "# Finally remove trailing spaces with strip()\n",
    "data['text'] = data.text.apply(lambda t : t.strip() )\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nY6RQnqSjbk3"
   },
   "source": [
    "Let's check out the resulting text for the different types:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "j7D5HfhXjZ8t",
    "outputId": "77b24bea-2d06-460e-dd13-ed9f66229658"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Does a quadratic log-likehood mean the MLE is approximately normally distributed?\n",
      "--------------------\n",
      "How to test if a value is over-represented in one sample vs another\n",
      "--------------------\n",
      "what is the Probability of selecting SNPs from a list of SNPs simply by chance\n"
     ]
    }
   ],
   "source": [
    "# Print examples of titles again - they should not be changed\n",
    "for p in data[data.category == 'title'].text.sample(3).values:\n",
    "    print('-' * 20)\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "_pyo6Bdhjl4J",
    "outputId": "4a185842-a6d7-4a3c-eaa0-51f86a5966f2"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "Being very new to recommender-systems and Matrix Factorization i was wondering how to get topN recommendations for a given user. So far my strategy is to create all possible User Item combinations, predict all of them and take the best N ones based on their predicted rating. This seems very inefficient to me as the number of possible combinations grows very fast. Question Is there a better method? If yes which one? In practice my approach by works as follows Having data as follows taken from movie lens in R-Package libFMexe User Movie Rating lt fctr gt lt fctr gt lt dbl gt Toy Story GoldenEye Toy Story Richard III Build all possible combinations test data.frame User factor levels levels dat User , Rating L User Movie Rating lt fctr gt lt fctr gt lt int gt GoldenEye Toy Story Richard III GoldenEye Toy Story Richard III Finally predict all user movie combinations and take the topN ratings per User as predictions. require libFMexe libFM dat, test, Rating User Movie, task r , dim , iter\n",
      "--------------------\n",
      "The split file function allows you to do this. It will calculate the correlation for every distinct level of the variable profession . So you'd run your overall correlation first without using split file , and then proceed with the split file, rerun your correlation, and there you go.\n",
      "--------------------\n",
      "You sample N of N items with replacement. How do you calculate the expected percent not sampled from original population N ? Extra Credit Generalize to sampling k of N items with replacement.\n"
     ]
    }
   ],
   "source": [
    "# Print examples of posts again - they should have much less clutter\n",
    "for p in data[data.category == 'post'].text.sample(3).values:\n",
    "    print('-' * 20)\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 138
    },
    "colab_type": "code",
    "id": "KKeBjpmsjxjZ",
    "outputId": "7301b185-7987-4503-8f63-20a1eb715962",
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------\n",
      "A kernel is NOT a mapping into feature space. It's a function that computes inner products in feature space. One could say that the choice of kernel implicitly determines a feature space mapping, but this is different than the kernel itself being such a mapping.\n",
      "--------------------\n",
      "metafor rma is a specific function in R. Could you more generally explain your model and how you currently calculate SE?\n",
      "--------------------\n",
      "you can only say over represented if you know define what it means to be normally represented. It’s much like a question about outliers first you have to decide what’s an in-lier. Statistics doesn’t do it for you.\n"
     ]
    }
   ],
   "source": [
    "# Print examples of comments again - should also be less noisy\n",
    "for p in data[data.category == 'comment'].text.sample(3).values:\n",
    "    print('-' * 20)\n",
    "    print(p)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "E4T1CQqWk6rP"
   },
   "source": [
    "# Tokenize\n",
    "\n",
    "Let's tokenize the text. \n",
    "This will allow us to count the number of tokens of each text and subsequently remove test that are too long or too short.\n",
    "You can use other librairies to tokenize the text (spacy for instance) or other tokenizer. Here we use the [WordPunctTokenizer](https://www.nltk.org/api/nltk.tokenize.html#nltk.tokenize.regexp.WordPunctTokenizer) from NLTK.\n",
    "\n",
    "And we create a new columns called tokens\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "2ge5LTc3j4Te"
   },
   "outputs": [],
   "source": [
    "#Create a tokenizer object using WordPunctTokenizer from NLTK \n",
    "tokenizer = WordPunctTokenizer()\n",
    "#Apply the tokenizer to the text column (conver to lowercase first) and save output into new column called tokens\n",
    "data['tokens'] = data.text.apply(lambda t : tokenizer.tokenize(t.lower())) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "2Ll09OAjmGby"
   },
   "source": [
    "Let's now count the tokens in each piece of text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "r3dp7L-Hl-AR"
   },
   "outputs": [],
   "source": [
    "#Count the number of tokens in each text and save output into new column called n_tokens\n",
    "data['n_tokens'] = data.tokens.apply(len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 168
    },
    "colab_type": "code",
    "id": "dzIE6rdWmPCo",
    "outputId": "40670ac8-d6c1-4909-e7fa-128630266d34"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    812132.000000\n",
       "mean         60.074186\n",
       "std          99.416031\n",
       "min           0.000000\n",
       "25%          16.000000\n",
       "50%          35.000000\n",
       "75%          70.000000\n",
       "max       10874.000000\n",
       "Name: n_tokens, dtype: float64"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Describe the number of tokens\n",
    "data.n_tokens.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 282
    },
    "colab_type": "code",
    "id": "PrwoRVQnmRa3",
    "outputId": "9b208aa6-0e3b-48d7-e15f-417e4b6536ff"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x12eb58e48>"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Do a histplot to check the distribution of the number of tokens\n",
    "data.n_tokens.hist(bins = 100)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "oJGNrR5wmV5r"
   },
   "source": [
    "We see that we have some extremely long texts. Let's look at the largest one"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "SgsxoSdWmUe3",
    "outputId": "6a72f8d1-d2ac-45b7-b1c9-0e669b025187"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "My sample includes subjects, of which belong to group L , while the other to group L please see data below . I used GLM for a binary outcome to test for group differences in background variables - summary pre lt - glm L g a m p e, family binomial logit , data df ...yielding significant differences for of them g, a, m, p, and e . So I modeled these background variables as covariates when testing for an association between my predictor, chr and my outcome rsk , in each one of the groups L , L , again using GLM for binary outcome summary fit lt - glm rsk chr g a m p e, family binomial logit , data df which df L , The results showed that a significant association does exist for L but not for L . I would appreciate your help in how to test whether significance non-significance can be attributed to the group condition? . Or in other words, is it true that for subjects L , a significant correlation is evident, while for L ' it's absent. Thanks for responders! Uri structure list L c , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , g c L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L , a c L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L , m c , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , p c , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , e c L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L , chr c L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L , rsk c , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , , row.names c L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L, L , class data.frame\n"
     ]
    }
   ],
   "source": [
    "# Print the largest token\n",
    "print(data[data.n_tokens > 10000].text.values[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "nK4wiFJamvac"
   },
   "source": [
    "We can see that most of the longest texts are composed of tables with limited semantic value. \n",
    "We will remove rows that have more than an arbitrary number of tokens (let's say 5000) as well as rows that have too few tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "cw8QXJ9CmjTo",
    "outputId": "5466e0dd-77c2-45ea-bd71-796063abbbe4"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(789649, 7)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Remove rows with less than 4 or more than 5000 tokens\n",
    "data = data[(data.n_tokens > 4) & (data.n_tokens < 5000)].reset_index(drop = True)\n",
    "#Check the number of rows after filtering\n",
    "data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 84
    },
    "colab_type": "code",
    "id": "P2IOaPeKsDHO",
    "outputId": "7a2883b6-6a39-48ac-df35-fec8290ebe4f"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "comment    540587\n",
       "post       165377\n",
       "title       83685\n",
       "Name: category, dtype: int64"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Lets check the value counts for each category again\n",
    "data.category.value_counts()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Fe4WDsdFnQ29"
   },
   "source": [
    "# Export data\n",
    "We could export the dataframe as such using a pickle file format. \n",
    "\n",
    "However if we want to keep the original csv format it's going to be easier if we transform the list of tokens into a space separated string.\n",
    "\n",
    "On retrieval we will only have to split the string to get back the list of tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 118
    },
    "colab_type": "code",
    "id": "vgqfut5snKj4",
    "outputId": "91673f01-cb41-45ee-f755-60a15efb3ade"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    the condition makes the gradient unbiased . it...\n",
       "1                       yes , that sounds fine to me .\n",
       "2    consider gaussian variables belonging to a gau...\n",
       "3    thanks s . catterall . - integrability i knew ...\n",
       "4                 feature with very few extreme values\n",
       "Name: tokens, dtype: object"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Change tokens column to a space separated string of tokens rather a list\n",
    "data['tokens'] = data.tokens.apply(lambda tk : ' '.join(tk))\n",
    "#Check the first few rows\n",
    "data.tokens.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And finally let's export the dataframe into a csv file.\n",
    "\n",
    "We will use that csv file as the new cleaned and tokenized dataset to build our language model in milestone 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Write to csv to upload\n",
    "data.to_csv('stackexchange_812k_tokenized.csv', quoting = csv.QUOTE_ALL, index = False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
